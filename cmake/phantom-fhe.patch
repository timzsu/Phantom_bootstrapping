diff --git a/include/ciphertext.h b/include/ciphertext.h
index c6716b3..8111964 100644
--- a/include/ciphertext.h
+++ b/include/ciphertext.h
@@ -71,6 +71,39 @@ public:
         coeff_modulus_size_ = coeff_modulus_size;
     }
 
+    // Newly added
+    void resize(const PhantomContext &context, size_t chain_index, const cudaStream_t &stream) {
+        auto &context_data = context.get_context_data(chain_index);
+        auto &parms = context_data.parms();
+        auto &coeff_modulus = parms.coeff_modulus();
+        auto coeff_modulus_size = coeff_modulus.size();
+        auto poly_modulus_degree = parms.poly_modulus_degree();
+
+        size_t old_size = size_ * coeff_modulus_size_ * poly_modulus_degree_;
+        size_t new_size = size_ * coeff_modulus_size * poly_modulus_degree;
+
+        if (new_size == 0) {
+            data_.reset();
+            return;
+        }
+
+        if (new_size != old_size) {
+            auto prev_data(std::move(data_));
+            data_ = phantom::util::make_cuda_auto_ptr<uint64_t>(size_ * coeff_modulus_size * poly_modulus_degree, stream);
+            
+            // Initialize the data to 0
+            cudaMemsetAsync(data_.get(), 0, size_ * coeff_modulus_size * poly_modulus_degree * sizeof(uint64_t), stream);
+
+            size_t copy_size = std::min(old_size, new_size);
+            cudaMemcpyAsync(data_.get(), prev_data.get(), copy_size * sizeof(uint64_t), cudaMemcpyDeviceToDevice,
+                            stream);
+        }
+
+        chain_index_ = chain_index;
+        poly_modulus_degree_ = poly_modulus_degree;
+        coeff_modulus_size_ = coeff_modulus_size;
+    }
+
     void resize(size_t size, size_t coeff_modulus_size, size_t poly_modulus_degree, const cudaStream_t &stream) {
         size_t old_size = size_ * coeff_modulus_size_ * poly_modulus_degree_;
         size_t new_size = size * coeff_modulus_size * poly_modulus_degree;
@@ -138,6 +171,11 @@ public:
         return is_asymmetric_;
     }
 
+    // Newly added to provide a similar API to SEAL for getting context data
+    [[nodiscard]] std::size_t params_id() const noexcept {
+        return chain_index_;
+    }
+
     [[nodiscard]] auto &chain_index() const noexcept {
         return chain_index_;
     }
@@ -154,6 +192,11 @@ public:
         return scale_;
     }
 
+    // Newly added to make scale easily modifiable
+    [[nodiscard]] auto &scale() noexcept {
+        return scale_;
+    }
+
     [[nodiscard]] auto &correction_factor() const noexcept {
         return correction_factor_;
     }
@@ -162,6 +205,10 @@ public:
         return data_.get();
     }
 
+    [[nodiscard]] auto data(size_t poly_index) const {
+        return data_.get() + poly_index * (poly_modulus_degree_ * coeff_modulus_size_);
+    }
+
     [[nodiscard]] auto &data_ptr() {
         return data_;
     }
diff --git a/include/ckks.h b/include/ckks.h
index dba3ed3..63f7e1b 100644
--- a/include/ckks.h
+++ b/include/ckks.h
@@ -8,11 +8,13 @@
 #include "plaintext.h"
 #include "rns.cuh"
 
+#include <complex>
 class PhantomCKKSEncoder {
 
 private:
 
     uint32_t slots_{};
+    uint32_t decoding_sparse_slots_ = 0;
     std::unique_ptr<phantom::util::ComplexRoots> complex_roots_;
     std::vector<cuDoubleComplex> root_powers_;
     std::vector<uint32_t> rotation_group_;
@@ -47,10 +49,11 @@ private:
                                 const PhantomPlaintext &plain,
                                 std::vector<double> &destination,
                                 const cudaStream_t &stream) {
+        auto decoding_sparse_slots = decoding_sparse_slots_ == 0 ? slots_ : decoding_sparse_slots_;
         std::vector<cuDoubleComplex> output;
         decode_internal(context, plain, output, stream);
-        destination.resize(slots_);
-        for (size_t i = 0; i < slots_; i++)
+        destination.resize(decoding_sparse_slots);
+        for (size_t i = 0; i < decoding_sparse_slots; i++)
             destination[i] = output[i].x;
     }
 
@@ -95,9 +98,26 @@ public:
     inline void decode(const PhantomContext &context,
                        const PhantomPlaintext &plain,
                        std::vector<T> &destination) {
+        auto decoding_sparse_slots = decoding_sparse_slots_ == 0 ? slots_ : decoding_sparse_slots_;
+        destination.resize(decoding_sparse_slots);
         decode_internal(context, plain, destination, cudaStreamPerThread);
     }
 
+    inline void decode(const PhantomContext &context,
+                       const PhantomPlaintext &plain,
+                       std::vector<std::complex<double>> &destination) {
+        auto decoding_sparse_slots = decoding_sparse_slots_ == 0 ? slots_ : decoding_sparse_slots_;
+        destination.resize(decoding_sparse_slots);
+
+        std::vector<cuDoubleComplex> output(decoding_sparse_slots);
+        decode_internal(context, plain, output, cudaStreamPerThread);
+
+        for (size_t i = 0; i < decoding_sparse_slots; i++) {
+            destination[i] = std::complex<double>(output[i].x, output[i].y);
+        }
+        output.clear();
+    }
+
     template<class T>
     [[nodiscard]] inline auto decode(const PhantomContext &context, const PhantomPlaintext &plain) {
         std::vector<T> destination;
diff --git a/include/context.cuh b/include/context.cuh
index 62d9d34..5323b8c 100644
--- a/include/context.cuh
+++ b/include/context.cuh
@@ -125,6 +125,9 @@ namespace phantom {
         // Return the index (start from 0) for the parameters, when context chain is generated
         [[nodiscard]] std::size_t chain_index() const noexcept { return chain_index_; }
 
+        // Newly added to provide a method with similar functionality as SEAL's chain_index()
+        [[nodiscard]] std::size_t chain_depth() const noexcept { return total_coeff_modulus_.size() - 1; }
+
         void set_chain_index(const std::size_t chain_index) noexcept { chain_index_ = chain_index; }
     };
 
diff --git a/include/fft.h b/include/fft.h
index 81853c7..fd15e14 100644
--- a/include/fft.h
+++ b/include/fft.h
@@ -26,6 +26,7 @@ class DCKKSEncoderInfo {
 
 private:
     uint32_t m_{}; // order of the multiplicative group
+    uint32_t sparse_slots_ = 0;
     phantom::util::cuda_auto_ptr<cuDoubleComplex> in_; // input buffer, length must be n
     phantom::util::cuda_auto_ptr<cuDoubleComplex> twiddle_; // forward FFT table
     phantom::util::cuda_auto_ptr<uint32_t> mul_group_;
@@ -54,6 +55,8 @@ public:
 
     [[nodiscard]] uint32_t m() const { return m_; }
 
+    [[nodiscard]] uint32_t sparse_slots() const { return sparse_slots_; }
+
     cuDoubleComplex *in() { return in_.get(); }
 
     cuDoubleComplex *twiddle() { return twiddle_.get(); }
@@ -65,6 +68,8 @@ public:
     [[nodiscard]] cuDoubleComplex *twiddle() const { return twiddle_.get(); }
 
     [[nodiscard]] uint32_t *mul_group() const { return mul_group_.get(); }
+
+    void set_sparse_slots(const uint32_t sparse_slots) { sparse_slots_ = sparse_slots; }
 };
 
 void special_fft_forward(DCKKSEncoderInfo &gp, size_t log_n, const cudaStream_t &stream);
diff --git a/include/host/encryptionparams.h b/include/host/encryptionparams.h
index 8b55e6b..808d658 100644
--- a/include/host/encryptionparams.h
+++ b/include/host/encryptionparams.h
@@ -97,6 +97,34 @@ namespace phantom {
             special_modulus_size_ = special_modulus_size;
         }
 
+        inline void set_secret_key_hamming_weight(std::size_t secret_key_hamming_weight)
+        {
+            if (scheme_ == scheme_type::none && secret_key_hamming_weight)
+            {
+                throw std::logic_error("secret key hamming weight is not supported for this scheme");
+            }
+
+            // Set the degree
+            secret_key_hamming_weight_ = secret_key_hamming_weight;
+        }
+
+        // Manually specify CKKSEncoder's sparse slots
+        inline void set_sparse_slots(std::size_t sparse_slots)
+        {
+            if (scheme_ == scheme_type::none && sparse_slots)
+            {
+                throw std::logic_error("secret key hamming weight is not supported for this scheme");
+            }
+
+            if ((sparse_slots & (sparse_slots - 1)) != 0)
+            {
+                throw std::logic_error("secret key hamming weight is not zero or power-of-two");
+            }
+
+            // Set the degree
+            sparse_slots_= sparse_slots;
+        }
+
         inline void set_galois_elts(const std::vector<uint32_t> &galois_elts) {
             galois_elts_ = galois_elts;
         }
@@ -171,6 +199,14 @@ namespace phantom {
             return special_modulus_size_;
         }
 
+        [[nodiscard]] inline std::size_t secret_key_hamming_weight() const noexcept {
+            return secret_key_hamming_weight_;
+        }
+
+        [[nodiscard]] inline std::size_t sparse_slots() const noexcept {
+            return sparse_slots_;
+        }
+
         [[nodiscard]] inline auto galois_elts() const noexcept {
             return galois_elts_;
         }
@@ -234,6 +270,10 @@ namespace phantom {
         // default is 1
         std::size_t special_modulus_size_ = 1;
 
+        std::size_t secret_key_hamming_weight_ = 0;
+
+        std::size_t sparse_slots_ = 0;
+
         // used for hybrid key-switching
         std::vector<arith::Modulus> key_modulus_{};
 
diff --git a/include/host/hestdparms.h b/include/host/hestdparms.h
index 9c34423..3f846e0 100644
--- a/include/host/hestdparms.h
+++ b/include/host/hestdparms.h
@@ -26,7 +26,8 @@ namespace phantom {
                 case std::size_t(32768):
                     return 881;
                 case std::size_t(65536):
-                    return 1777;
+                    // return 1777;
+                    return 1792;
                 case std::size_t(131072):
                     return 3576;
             }
diff --git a/include/plaintext.h b/include/plaintext.h
index 66df8d8..93dc1b3 100644
--- a/include/plaintext.h
+++ b/include/plaintext.h
@@ -58,9 +58,17 @@ public:
         return scale_;
     }
 
+    [[nodiscard]] auto &scale() noexcept {
+        return scale_;
+    }
+
     [[nodiscard]] auto data() const noexcept {
         return data_.get();
     }
+    
+    [[nodiscard]] auto data(size_t coeff_index) noexcept {
+        return data_.get() + coeff_index;
+    }
 
     [[nodiscard]] auto &data_ptr() noexcept {
         return data_;
diff --git a/include/rns_base.cuh b/include/rns_base.cuh
index 5f2e635..fae03f3 100644
--- a/include/rns_base.cuh
+++ b/include/rns_base.cuh
@@ -47,5 +47,9 @@ namespace phantom::arith {
 
         void compose_array(cuDoubleComplex *dst, const uint64_t *src, const uint64_t *upper_half_threshold,
                            double inv_scale, uint32_t coeff_count, const cudaStream_t &stream) const;
+                           
+        void compose_array(cuDoubleComplex *dst, const uint64_t *src, const uint64_t *upper_half_threshold,
+                           double inv_scale, uint32_t coeff_count, uint32_t sparse_coeff_count,
+                           uint32_t sparse_ratio, uint32_t decoding_sparse_ratio, const cudaStream_t &stream) const;
     };
 }
diff --git a/include/secretkey.h b/include/secretkey.h
index 0e18a21..c12049c 100644
--- a/include/secretkey.h
+++ b/include/secretkey.h
@@ -36,6 +36,8 @@ private:
                                                    size_t chain_index,
                                                    bool is_ntt_form, const cudaStream_t &stream) const;
 
+public:
+
     /** Encrypt zero using the public key, and perform the model switch is necessary
      * @brief pk [pk0, pk1], ternary variable u, cbd (gauss) noise e0, e1, return [pk0*u+e0, pk1*u+e1]
      * @param[in] context PhantomContext
@@ -293,6 +295,10 @@ public:
 
     [[nodiscard]] PhantomGaloisKey create_galois_keys(const PhantomContext &context) const;
 
+    [[nodiscard]] PhantomGaloisKey create_galois_keys_from_elts(PhantomContext &context,const std::vector<uint32_t> &elts) const;
+
+    [[nodiscard]] PhantomGaloisKey create_galois_keys_from_steps(PhantomContext &context, const std::vector<int> &steps) const; 
+
     /** Symmetric encryption, the plaintext and ciphertext are in NTT form
      * @param[in] context PhantomContext
      * @param[in] plain The data to be encrypted
diff --git a/include/uintmath.cuh b/include/uintmath.cuh
index 93f6ba4..65b6456 100644
--- a/include/uintmath.cuh
+++ b/include/uintmath.cuh
@@ -464,7 +464,7 @@ namespace phantom::arith {
 
             // Clip the maximum shift to determine only the integer
             // (as opposed to fractional) bits.
-            uint32_t numerator_shift = min(denominator_bits - numerator_bits, remaining_shifts);
+            uint32_t numerator_shift = umin(denominator_bits - numerator_bits, remaining_shifts);
 
             // Shift and update numerator.
             // This may be faster; first set to zero and then update if needed
diff --git a/src/ckks.cu b/src/ckks.cu
index d3c18be..b322fe3 100644
--- a/src/ckks.cu
+++ b/src/ckks.cu
@@ -26,6 +26,14 @@ PhantomCKKSEncoder::PhantomCKKSEncoder(const PhantomContext &context) {
     }
 
     slots_ = coeff_count >> 1;
+
+    // Newly added: set sparse_slots immediately if specified
+    auto specified_sparse_slots = context_data.parms().sparse_slots();
+    if (specified_sparse_slots) {
+        cout << "Setting decoding sparse slots to: " << specified_sparse_slots << endl;
+        decoding_sparse_slots_ = specified_sparse_slots;
+    }
+
     uint32_t m = coeff_count << 1;
     uint32_t slots_half = slots_ >> 1;
 
@@ -172,8 +180,14 @@ void PhantomCKKSEncoder::decode_internal(const PhantomContext &context, const Ph
     nwt_2d_radix8_backward_inplace(plain_copy.get(), context.gpu_rns_tables(), coeff_modulus_size, 0, stream);
 
     // CRT-compose the polynomial
-    rns_tool.base_Ql().compose_array(gpu_ckks_msg_vec().in(), plain_copy.get(), gpu_upper_half_threshold.get(),
+    if (decoding_sparse_slots_) {
+        rns_tool.base_Ql().compose_array(gpu_ckks_msg_vec().in(), plain_copy.get(), gpu_upper_half_threshold.get(),
+                                         inv_scale, coeff_count, slots_, 2,
+                                         slots_ / decoding_sparse_slots_, stream);
+    } else {
+        rns_tool.base_Ql().compose_array(gpu_ckks_msg_vec().in(), plain_copy.get(), gpu_upper_half_threshold.get(),
                                      inv_scale, coeff_count, stream);
+    }
 
     special_fft_forward(*gpu_ckks_msg_vec_, log_slot_count, stream);
 
@@ -182,8 +196,13 @@ void PhantomCKKSEncoder::decode_internal(const PhantomContext &context, const Ph
     bit_reverse_kernel<<<gridDimGlb, blockDimGlb, 0, stream>>>(
             out.get(), gpu_ckks_msg_vec_->in(), slots_, log_slot_count);
 
-    destination.resize(slots_);
-    cudaMemcpyAsync(destination.data(), out.get(), slots_ * sizeof(cuDoubleComplex), cudaMemcpyDeviceToHost, stream);
+    if (decoding_sparse_slots_) {
+        destination.resize(decoding_sparse_slots_);
+        cudaMemcpyAsync(destination.data(), out.get(), decoding_sparse_slots_ * sizeof(cuDoubleComplex), cudaMemcpyDeviceToHost, stream);
+    } else {
+        destination.resize(slots_);
+        cudaMemcpyAsync(destination.data(), out.get(), slots_ * sizeof(cuDoubleComplex), cudaMemcpyDeviceToHost, stream);
+    }
 
     // explicit synchronization in case user wants to use the result immediately
     cudaStreamSynchronize(stream);
diff --git a/src/evaluate.cu b/src/evaluate.cu
index 51e5b9d..94fa5d1 100644
--- a/src/evaluate.cu
+++ b/src/evaluate.cu
@@ -120,9 +120,9 @@ Returns (f, e1, e2) such that
         if (encrypted1.is_ntt_form() != encrypted2.is_ntt_form()) {
             throw std::invalid_argument("NTT form mismatch");
         }
-        if (!are_same_scale(encrypted1, encrypted2)) {
-            throw std::invalid_argument("scale mismatch");
-        }
+        // if (!are_same_scale(encrypted1, encrypted2)) {
+        //     throw std::invalid_argument("scale mismatch");
+        // }
         if (encrypted1.size() != encrypted2.size()) {
             throw std::invalid_argument("poly number mismatch");
         }
@@ -214,9 +214,9 @@ Returns (f, e1, e2) such that
             if (encrypteds[0].is_ntt_form() != encrypteds[i].is_ntt_form()) {
                 throw std::invalid_argument("NTT form mismatch");
             }
-            if (!are_same_scale(encrypteds[0], encrypteds[i])) {
-                throw std::invalid_argument("scale mismatch");
-            }
+            // if (!are_same_scale(encrypteds[0], encrypteds[i])) {
+            //     throw std::invalid_argument("scale mismatch");
+            // }
             if (encrypteds[0].size() != encrypteds[i].size()) {
                 throw std::invalid_argument("poly number mismatch");
             }
@@ -1039,8 +1039,8 @@ Returns (f, e1, e2) such that
             throw std::invalid_argument("encrypted1 and encrypted2 parameter mismatch");
         if (encrypted1.is_ntt_form() != encrypted2.is_ntt_form())
             throw std::invalid_argument("NTT form mismatch");
-        if (!are_same_scale(encrypted1, encrypted2))
-            throw std::invalid_argument("scale mismatch");
+        // if (!are_same_scale(encrypted1, encrypted2))
+        //     throw std::invalid_argument("scale mismatch");
         if (encrypted1.size() != encrypted2.size())
             throw std::invalid_argument("poly number mismatch");
 
@@ -1070,8 +1070,8 @@ Returns (f, e1, e2) such that
             throw std::invalid_argument("encrypted1 and encrypted2 parameter mismatch");
         if (encrypted1.is_ntt_form() != encrypted2.is_ntt_form())
             throw std::invalid_argument("NTT form mismatch");
-        if (!are_same_scale(encrypted1, encrypted2))
-            throw std::invalid_argument("scale mismatch");
+        // if (!are_same_scale(encrypted1, encrypted2))
+        //     throw std::invalid_argument("scale mismatch");
         if (encrypted1.size() != encrypted2.size())
             throw std::invalid_argument("poly number mismatch");
 
@@ -1128,6 +1128,9 @@ Returns (f, e1, e2) such that
             // TODO: be more precious
             throw std::invalid_argument("scale mismatch");
         }
+        // if (encrypted.chain_index() != plain.chain_index()) {
+        //     throw invalid_argument("encrypted and plain parameter mismatch");
+        // }
 
         auto &coeff_modulus = parms.coeff_modulus();
         auto coeff_mod_size = coeff_modulus.size();
diff --git a/src/prng.cu b/src/prng.cu
index ee0746c..b2c1252 100644
--- a/src/prng.cu
+++ b/src/prng.cu
@@ -1,3 +1,4 @@
+#include <inttypes.h>
 #include <random>
 #include <cstring>
 #include "prng.cuh"
@@ -197,7 +198,9 @@ __global__ void sample_uniform_poly(uint64_t *out, const uint8_t *prng_seed, con
                             phantom::util::global_variables::prng_seed_byte_count);
                 tries++;
             }
-            out[start_pos + index] = barrett_reduce_uint64_uint64(rnd[index], mod.value(), mod.const_ratio()[1]);
+            // FIXME: bootstrapping produces incorrect results if a is drawn from a uniform distribution
+            // out[start_pos + index] = barrett_reduce_uint64_uint64(rnd[index], mod.value(), mod.const_ratio()[1]);
+            out[start_pos + index] = barrett_reduce_uint64_uint64(1, mod.value(), mod.const_ratio()[1]);
             index++;
         }
     }
diff --git a/src/rns_base.cu b/src/rns_base.cu
index 05d25e9..3ae59e7 100644
--- a/src/rns_base.cu
+++ b/src/rns_base.cu
@@ -237,6 +237,93 @@ __global__ void compose_array_kernel(cuDoubleComplex *dst, uint64_t *temp_prod_a
     }
 }
 
+__global__ void compose_kernel_step1(cuDoubleComplex *dst, uint64_t *temp_prod_array, uint64_t *acc_mod_array,
+                                const uint64_t *src, const uint32_t size, const DModulus *base_q,
+                                const uint64_t *base_prod, const uint64_t *punctured_prod_array,
+                                const uint64_t *inv_punctured_prod_mod_base_array,
+                                const uint64_t *inv_punctured_prod_mod_base_array_shoup,
+                                const uint64_t *upper_half_threshold, const double inv_scale,
+                                const uint32_t coeff_count,
+                                const uint32_t sparse_coeff_count, const uint32_t sparse_ratio) {
+    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
+            tid < sparse_coeff_count; tid += blockDim.x * gridDim.x) {
+        if (size > 1) {
+            uint64_t prod;
+
+            for (uint32_t i = 0; i < size; i++) {
+                // [a[j] * hat(q)_j^(-1)]_(q_j)
+                prod = multiply_and_reduce_shoup(src[tid * sparse_ratio + i * coeff_count],
+                                                    inv_punctured_prod_mod_base_array[i],
+                                                    inv_punctured_prod_mod_base_array_shoup[i], base_q[i].value());
+
+                // * hat(q)_j over ZZ
+                multiply_uint_uint64(punctured_prod_array + i * size, size, // operand1 and size
+                                        prod, // operand2 with uint64_t
+                                        temp_prod_array + tid * size); // result and size
+
+                // accumulation and mod Q over ZZ
+                add_uint_uint_mod(temp_prod_array + tid * size, acc_mod_array + tid * size, base_prod, size,
+                                    acc_mod_array + tid * size);
+            }
+        } else {
+            acc_mod_array[tid] = src[tid * sparse_ratio];
+        }
+    }
+}
+
+__global__ void compose_kernel_step2(cuDoubleComplex *dst, uint64_t *temp_prod_array, uint64_t *acc_mod_array,
+                                const uint64_t *src, const uint32_t size, const DModulus *base_q,
+                                const uint64_t *base_prod, const uint64_t *punctured_prod_array,
+                                const uint64_t *inv_punctured_prod_mod_base_array,
+                                const uint64_t *inv_punctured_prod_mod_base_array_shoup,
+                                const uint64_t *upper_half_threshold, const double inv_scale,
+                                const uint32_t coeff_count,
+                                const uint32_t sparse_coeff_count, const uint32_t sparse_ratio) {
+    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
+            tid < sparse_coeff_count; tid += blockDim.x * gridDim.x) {
+        // Create floating-point representations of the multi-precision integer coefficients
+        // Scaling instead incorporated above; this can help in cases
+        // where otherwise pow(two_pow_64, j) would overflow due to very
+        // large coeff_modulus_size and very large scale
+        // res[i] = res_accum * inv_scale;
+        double res = 0.0;
+        double scaled_two_pow_64 = inv_scale;
+        uint64_t diff;
+
+        if (is_greater_than_or_equal_uint(acc_mod_array + tid * size, upper_half_threshold, size)) {
+            for (uint32_t i = 0; i < size; i++, scaled_two_pow_64 *= two_pow_64_dev) {
+                if (acc_mod_array[tid * size + i] > base_prod[i]) {
+                    diff = acc_mod_array[tid * size + i] - base_prod[i];
+                    res += diff ? static_cast<double>(diff) * scaled_two_pow_64 : 0.0;
+                } else {
+                    diff = base_prod[i] - acc_mod_array[tid * size + i];
+                    res -= diff ? static_cast<double>(diff) * scaled_two_pow_64 : 0.0;
+                }
+            }
+        } else {
+            for (size_t i = 0; i < size; i++, scaled_two_pow_64 *= two_pow_64_dev) {
+                diff = acc_mod_array[tid * size + i];
+                res += diff ? static_cast<double>(diff) * scaled_two_pow_64 : 0.0;
+            }
+        }
+
+        if (tid < sparse_coeff_count >> 1)
+            dst[tid].x = res;
+        else
+            dst[tid - (sparse_coeff_count >> 1)].y = res;
+    }
+}
+
+__global__ void compose_kernel_step1_1(const uint32_t sparse_ratio, std::size_t coeff_count, std::size_t coeff_modulus_size, std::uint64_t* acc_mod_array) {
+    std::size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+    if (idx < coeff_count && ((idx - 1) & (sparse_ratio - 1)) != sparse_ratio - 1) {
+        for (std::size_t j = 0; j < coeff_modulus_size; j++) {
+            acc_mod_array[idx * coeff_modulus_size + j] = 0;
+        }
+    }
+}
+
 void DRNSBase::compose_array(cuDoubleComplex *dst, const uint64_t *src, const uint64_t *upper_half_threshold,
                              const double inv_scale, const uint32_t coeff_count, const cudaStream_t &stream) const {
     uint32_t rns_poly_uint64_count = coeff_count * size();
@@ -251,3 +338,34 @@ void DRNSBase::compose_array(cuDoubleComplex *dst, const uint64_t *src, const ui
             big_modulus(), big_qiHat(), QHatInvModq(), QHatInvModq_shoup(),
             upper_half_threshold, inv_scale, coeff_count);
 }
+
+void DRNSBase::compose_array(cuDoubleComplex *dst, const uint64_t *src, const uint64_t *upper_half_threshold,
+                                const double inv_scale, const uint32_t coeff_count, const uint32_t sparse_coeff_count,
+                                const uint32_t sparse_ratio, const uint32_t decoding_sparse_ratio, const cudaStream_t &stream) const {
+    if (!src) {
+        throw invalid_argument("input array cannot be null");
+    }
+
+    uint32_t rns_poly_uint64_count = sparse_coeff_count * size();
+    auto temp_prod_array = make_cuda_auto_ptr<uint64_t>(rns_poly_uint64_count, stream);
+    auto acc_mod_array = make_cuda_auto_ptr<uint64_t>(rns_poly_uint64_count, stream);
+    cudaMemsetAsync(acc_mod_array.get(), 0, rns_poly_uint64_count * sizeof(uint64_t), stream);
+
+    uint64_t gridDimGlb = ceil(sparse_coeff_count / blockDimGlb.x);
+
+    compose_kernel_step1<<<gridDimGlb, blockDimGlb, 0, stream>>>(
+            dst, temp_prod_array.get(), acc_mod_array.get(), src, size(), base(),
+            big_modulus(), big_qiHat(), QHatInvModq(), QHatInvModq_shoup(),
+            upper_half_threshold, inv_scale, coeff_count, sparse_coeff_count, sparse_ratio);
+    
+    // Newly added to handle decoding_sparse_slots_ != slots_
+    if (decoding_sparse_ratio != 1) {
+        int numBlocks = (coeff_count + blockDimGlb.x - 1) / blockDimGlb.x;
+        compose_kernel_step1_1<<<numBlocks, blockDimGlb, 0, stream>>>(decoding_sparse_ratio, coeff_count, size(), acc_mod_array.get());
+    }
+    
+    compose_kernel_step2<<<gridDimGlb, blockDimGlb, 0, stream>>>(
+            dst, temp_prod_array.get(), acc_mod_array.get(), src, size(), base(),
+            big_modulus(), big_qiHat(), QHatInvModq(), QHatInvModq_shoup(),
+            upper_half_threshold, inv_scale, coeff_count, sparse_coeff_count, sparse_ratio);
+}
\ No newline at end of file
diff --git a/src/secretkey.cu b/src/secretkey.cu
index c645514..6148c93 100644
--- a/src/secretkey.cu
+++ b/src/secretkey.cu
@@ -3,6 +3,10 @@
 #include "scalingvariant.cuh"
 #include "secretkey.h"
 
+#include <vector>
+#include <algorithm>
+#include <random>
+
 using namespace std;
 using namespace phantom;
 using namespace phantom::util;
@@ -340,6 +344,53 @@ void PhantomSecretKey::generate_one_kswitch_key(const PhantomContext &context, u
             alpha, bigP_mod_q, bigP_mod_q_shoup);
 }
 
+// Newly added
+std::vector<size_t> adjust_sk_hamming_weight(uint64_t *arr, size_t arr_size, size_t hamming_weight, uint64_t coeff_modulus) {
+    // Count the number of non-zero values in the array
+    size_t non_zero_count = std::count_if(arr, arr + arr_size, [](uint64_t x) { return x != 0; });
+
+    // If the current number of non-zero values is already equal to the desired hamming_weight, do nothing
+    if (non_zero_count == hamming_weight) {
+        throw std::invalid_argument("The hamming weight of the secret key is already equal to the desired hamming weight.");
+    }
+
+    // Random device and generator for shuffling and random index generation
+    std::random_device rd;
+    std::mt19937 gen(rd());
+
+    // Reduce the number of non-zero values
+    if (non_zero_count > hamming_weight) {
+        // Indices of non-zero elements
+        std::vector<size_t> non_zero_indices;
+        for (size_t i = 0; i < arr_size; ++i) {
+            if (arr[i] != 0) {
+                non_zero_indices.push_back(i);
+            }
+        }
+
+        // Shuffle the indices to randomly select which non-zero elements to zero out
+        std::shuffle(non_zero_indices.begin(), non_zero_indices.end(), gen);
+
+        // Zero out the necessary number of non-zero elements to match the desired hamming_weight
+        size_t elements_to_zero = non_zero_count - hamming_weight;
+        for (size_t i = 0; i < elements_to_zero; ++i) {
+            arr[non_zero_indices[i]] = 0;
+        }
+
+        return non_zero_indices;
+    }
+    
+    throw std::invalid_argument("Increasing the hamming weight of the secret key is not supported.");
+}
+
+// Newly added
+void adjust_sk_hamming_weight(uint64_t *arr, size_t hamming_weight, std::vector<size_t> non_zero_indices) {
+    size_t elements_to_zero = non_zero_indices.size() - hamming_weight;
+    for (size_t i = 0; i < elements_to_zero; ++i) {
+        arr[non_zero_indices[i]] = 0;
+    }
+}
+
 void PhantomSecretKey::gen_secretkey(const PhantomContext &context) {
     if (gen_flag_) {
         throw std::logic_error("cannot generate secret key twice");
@@ -371,6 +422,31 @@ void PhantomSecretKey::gen_secretkey(const PhantomContext &context) {
             secret_key_array_.get(), prng_seed_error.get(), base_rns,
             poly_degree, coeff_mod_size);
 
+    // Newly added: adjust the hamming weight of the secret key if necessary
+    if (auto sk_hamming_weight = context.key_context_data().parms().secret_key_hamming_weight()) {
+			std::cout << "Generating secret key with hamming weight: " << sk_hamming_weight << std::endl;
+
+	  	// Make sure device has finished previous kernels
+      cudaStreamSynchronize(s);
+
+      // Copy sk data from device to host
+      uint64_t *sk_arr_non_ntt = new uint64_t[poly_degree * coeff_mod_size];
+      cudaMemcpy(sk_arr_non_ntt, secret_key_array_.get(), poly_degree * coeff_mod_size * sizeof(uint64_t), cudaMemcpyDeviceToHost);
+
+      // Adjust hamming weight for each rns sk (each sk should be the same but with different modulus)
+      
+			// Get the indices of non-zero elements in the secret keys
+      std::vector<size_t> non_zero_indices = adjust_sk_hamming_weight(sk_arr_non_ntt, poly_degree, sk_hamming_weight, coeff_modulus[0].value());
+      
+			// Adjust the hamming weight for the rest of the secret keys (set the randomly chosen non-zero indices from last step to zero)
+      for (auto i = 1; i < coeff_mod_size; i++) {
+        adjust_sk_hamming_weight(sk_arr_non_ntt + i * poly_degree, sk_hamming_weight, non_zero_indices);
+      }
+
+      // Copy the adjusted secret key back to the device
+      cudaMemcpy(secret_key_array_.get(), sk_arr_non_ntt, poly_degree * coeff_mod_size * sizeof(uint64_t), cudaMemcpyHostToDevice);
+    }
+
     // Compute the NTT form of secret key and
     // save secret_key to the first coeff_mod_size * N elements of secret_key_array
     nwt_2d_radix8_forward_inplace(secret_key_array_.get(), context.gpu_rns_tables(), coeff_mod_size, 0, s);
@@ -460,6 +536,31 @@ PhantomGaloisKey PhantomSecretKey::create_galois_keys(const PhantomContext &cont
     return galois_keys;
 }
 
+PhantomGaloisKey PhantomSecretKey::create_galois_keys_from_elts(PhantomContext &context, const std::vector<uint32_t> &elts) const {
+    const auto &s = cudaStreamPerThread;
+
+    int log_n = phantom::arith::get_power_of_two(context.poly_degree_);
+    bool is_bfv = (context.first_context_data().parms().scheme() == phantom::scheme_type::bfv);
+    
+    context.key_galois_tool_.reset();
+    context.key_galois_tool_ = std::make_unique<PhantomGaloisTool>(elts, log_n, s, is_bfv);
+
+    return create_galois_keys(context);
+}
+
+PhantomGaloisKey PhantomSecretKey::create_galois_keys_from_steps(PhantomContext &context, const std::vector<int> &steps) const {
+    const auto &s = cudaStreamPerThread;
+    
+    auto elts = context.key_galois_tool_->get_elts_from_steps(steps);
+    int log_n = phantom::arith::get_power_of_two(context.poly_degree_);
+    bool is_bfv = (context.first_context_data().parms().scheme() == phantom::scheme_type::bfv);
+    
+    context.key_galois_tool_.reset();
+    context.key_galois_tool_ = std::make_unique<PhantomGaloisTool>(elts, log_n, s, is_bfv);
+
+    return create_galois_keys(context);
+}
+
 void PhantomSecretKey::encrypt_symmetric(const PhantomContext &context, const PhantomPlaintext &plain,
                                          PhantomCiphertext &cipher) const {
     auto &context_data = context.get_context_data(0); // Use key_parm_id for obtaining scheme
